{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import time\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = \"JwRDQeXze9PYfPVq5HTLqXVty\"\n",
    "consumer_secret = \"dbuGIfHlH8Z8T9pza4Va7HKP017C2BJenhRgCRsMho4t6uWqHo\"\n",
    "access_token = \"1308274051228467200-Gb848DLoDkfW6nExVQoZ3QDfiQnWpe\"\n",
    "access_token_secret = \"4JuOwiiuRoVNAAevr6w0ampCe7JYRtXM0y9miXoqQUFtU\"\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication OK\n"
     ]
    }
   ],
   "source": [
    "# Authentication testing\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "except:\n",
    "    print(\"Error during authentication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = \"JwRDQeXze9PYfPVq5HTLqXVty\"\n",
    "consumer_secret = \"dbuGIfHlH8Z8T9pza4Va7HKP017C2BJenhRgCRsMho4t6uWqHo\"\n",
    "access_token = \"1308274051228467200-Gb848DLoDkfW6nExVQoZ3QDfiQnWpe\"\n",
    "access_token_secret = \"4JuOwiiuRoVNAAevr6w0ampCe7JYRtXM0y9miXoqQUFtU\"\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://medium.com/@leowgriffin/scraping-tweets-with-tweepy-python-59413046e788\n",
    "# His recommended number of tweets per run is 2500\n",
    "def scraptweets(search_words, since_id, max_id, numTweets, numRuns, targetcsv):\n",
    "    \n",
    "    # Define a for-loop to generate tweets at regular intervals\n",
    "    # We cannot make large API call in one go. Hence, let's try T times\n",
    "    # Define a pandas dataframe to store the date:\n",
    "    db_tweets = pd.DataFrame(columns = ['username', 'text', 'tweetcreatedts', 'hashtags', 'retweetcount', 'likecount', 'acctdesc', \n",
    "                                        'location', 'followers', 'totaltweets',\n",
    "                                         'usercreatedts'])\n",
    "    program_start = time.time()\n",
    "    for i in range(0, numRuns):\n",
    "        # We will time how long it takes to scrape tweets for each run:\n",
    "        start_run = time.time()\n",
    "\n",
    "        # Collect tweets using the Cursor object\n",
    "        # .Cursor() returns an object that you can iterate or loop over to access the data collected.\n",
    "        # Each item in the iterator has various attributes that you can access to get information about each tweet\n",
    "        tweets = tweepy.Cursor(api.search, q=search_words, lang=\"en\", since_id = since_id, max_id = max_id, tweet_mode='extended').items(numTweets)\n",
    "        # Store these tweets into a python list\n",
    "        tweet_list = [tweet for tweet in tweets]\n",
    "        # Obtain the following info (methods to call them out):\n",
    "            # user.screen_name - twitter handle\n",
    "            # user.description - description of account\n",
    "            # user.location - where is he tweeting from\n",
    "            # user.followers_count - no. of other users who are following this user (followers)\n",
    "            # user.statuses_count - total tweets by user\n",
    "            # user.created_at - when the user account was created\n",
    "            # created_at - when the tweet was created\n",
    "            # favorite_count - how many likes the tweet received\n",
    "            # retweet_count - how many times this tweet was retweeted\n",
    "            # retweeted_status.full_text - full text of the tweet\n",
    "            # tweet.entities['hashtags'] - hashtags in the tweet\n",
    "        # Begin scraping the tweets individually:\n",
    "        noTweets = 0\n",
    "        for tweet in tweet_list:\n",
    "        # Pull the values\n",
    "                username = tweet.user.screen_name\n",
    "                acctdesc = tweet.user.description\n",
    "                location = tweet.user.location\n",
    "                followers = tweet.user.followers_count\n",
    "                totaltweets = tweet.user.statuses_count\n",
    "                usercreatedts = tweet.user.created_at\n",
    "                tweetcreatedts = tweet.created_at\n",
    "                retweetcount = tweet.retweet_count\n",
    "                likecount = tweet.favorite_count\n",
    "                hashtags = []\n",
    "                if len(tweet.entities['hashtags']) != 0:\n",
    "                    for i in range(0, len(tweet.entities['hashtags'])):\n",
    "                        hashtags.append(tweet.entities['hashtags'][i].get('text'))\n",
    "                else:\n",
    "                    pass\n",
    "                try:\n",
    "                    text = tweet.retweeted_status.full_text\n",
    "                except AttributeError:  # Not a Retweet\n",
    "                    text = tweet.full_text\n",
    "                # Add the 11 variables to the empty list - ith_tweet:\n",
    "                ith_tweet = [username, text, tweetcreatedts, hashtags, likecount, retweetcount, acctdesc, location, followers, totaltweets,\n",
    "                             usercreatedts]\n",
    "                # Append to dataframe - db_tweets\n",
    "                db_tweets.loc[len(db_tweets)] = ith_tweet\n",
    "                # increase counter - noTweets  \n",
    "                noTweets += 1\n",
    "\n",
    "        # Run ended:\n",
    "        end_run = time.time()\n",
    "        duration_run = round((end_run-start_run)/60, 2)\n",
    "\n",
    "        print('no. of tweets scraped for run {} is {}'.format(i + 1, noTweets))\n",
    "        print('time take for {} run to complete is {} mins'.format(i+1, duration_run))\n",
    "\n",
    "        #time.sleep(1200) #20 minute sleep time\n",
    "    # Once all runs have completed, save them to a single csv file:\n",
    "    db_tweets.to_csv(targetcsv, index = False)\n",
    "\n",
    "    program_end = time.time()\n",
    "    print('Scraping has completed!')\n",
    "    print('Total time taken to scrap is {} minutes.'.format(round(program_end - program_start)/60, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise these variables:\n",
    "search_words = \"#CHECRY OR Chelsea OR Crystal Palace -is:retweet\"\n",
    "date_since = \"2020-10-03\"\n",
    "date_until = \"2020-10-04\"\n",
    "# chelsea lineup post\n",
    "since_id = 1312380435415080960\n",
    "# chelsea fulltime post\n",
    "max_id = 1312383224316866565\n",
    "numTweets = 2000\n",
    "numRuns = 1\n",
    "targetcsv = '../datasets/chelseapalacetweets7.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of tweets scraped for run 2 is 1744\n",
      "time take for 2 run to complete is 1.47 mins\n",
      "Scraping has completed!\n",
      "Total time taken to scrap is 1.4666666666666666 minutes.\n"
     ]
    }
   ],
   "source": [
    "scraptweets(search_words, since_id, max_id, numTweets, numRuns, targetcsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Chelsea vs Crystal palace. \\n\\n‚Ä¢ Mendy clean s...\n",
       "1       Wait, this same Crystal Palace flogged Man Utd...\n",
       "2       Seems like beating Crystal Palace at home this...\n",
       "3       üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø Ben Chilwell vs Crystal Palace\\n\\n90 m...\n",
       "4       I love how Havertz constantly roams around the...\n",
       "                              ...                        \n",
       "4995    Seems like beating Crystal Palace at home this...\n",
       "4996    Beating Crystal Palace at home isn‚Äôt for every...\n",
       "4997    A chelsea fan wants us to play Crystal Palace ...\n",
       "4998    Beating Crystal Palace at home isn‚Äôt for every...\n",
       "4999    Today: Chelsea vs. Crystal Palace.\\n\\nüëâüëà https...\n",
       "Name: text, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('../datasets/chelseapalacetweets.csv')\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('../datasets/chelseapalacetweets2.csv')\n",
    "len(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "918"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('../datasets/chelseapalacetweets3.csv')\n",
    "df['text'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('../datasets/chelseapalacetweets5.csv')\n",
    "df['text'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "572"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('../datasets/chelseapalacetweets6.csv')\n",
    "df['text'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# half time\n",
    "https://twitter.com/ChelseaFC/status/1312366342184591370\n",
    "# kick off\n",
    "https://twitter.com/ChelseaFC/status/1312354735291273216\n",
    "# 1-0\n",
    "https://twitter.com/ChelseaFC/status/1312372192529780737\n",
    "# 2-0\n",
    "https://twitter.com/ChelseaFC/status/1312376306210877440\n",
    "# 3-0\n",
    "https://twitter.com/ChelseaFC/status/1312379117627740161\n",
    "# 4-0\n",
    "https://twitter.com/ChelseaFC/status/1312380435415080960\n",
    "# full tme\n",
    "https://twitter.com/ChelseaFC/status/1312383224316866565"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "\n",
    "https://towardsdatascience.com/how-to-scrape-tweets-from-twitter-59287e20f0f1\n",
    "\n",
    "https://towardsdatascience.com/how-to-scrape-more-information-from-tweets-on-twitter-44fd540b8a1f\n",
    "\n",
    "https://github.com/MartinBeckUT/TwitterScraper/tree/master/BasicScraper\n",
    "\n",
    "https://medium.com/analytics-vidhya/how-to-get-trending-tweets-in-any-country-with-python-and-tweepy-af2bfe760251"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
